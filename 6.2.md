## 6.2 데이터 레이크(Data Lake) 패턴

지난 수년간 웹, 모바일, IoT, 마이크로서비스 등의 다양한 소스에서 생성되는 데이터의 양과 형태는 기하급수적으로 증가하였습니다. 이러한 이질적이고 대규모의 데이터를 유연하게 저장하고 분석하기 위해, 많은 조직에서는 전통적인 데이터 웨어하우스 중심의 아키텍처에서 탈피하여 데이터 레이크(Data Lake) 기반의 아키텍처로 전환하고 있습니다. 본 절에서는 데이터 레이크의 개념, 구성 요소, 아키텍처 구현 방식 및 실전 적용 패턴에 대해 자세히 살펴보겠습니다.

### 데이터 레이크란 무엇인가

데이터 레이크는 구조화, 반정형, 그리고 비정형 데이터까지 다양한 데이터를 원시 포맷(raw format)으로 저장한 후 필요에 따라 처리하고 사용할 수 있는 중앙 저장소를 의미합니다. 이는 스키마 정의(즉, 데이터 구조 정리)를 사전에 요구하는 데이터 웨어하우스와 구별되며, "Schema-on-Read" 방식을 따릅니다. 즉, 데이터를 저장할 때 미리 스키마를 적용하는 대신, 데이터를 읽을 때 분석 관점에서 스키마를 적용하는 방식입니다.

데이터 레이크는 클라우드 기반의 대용량 객체 스토리지(ex: Amazon S3, Azure Data Lake Storage, Google Cloud Storage)를 기반으로 구축되는 경우가 많으며, 유연한 확장성과 높은 내구성, 그리고 낮은 비용 구조를 제공합니다.

### 데이터 레이크 구성 요소

데이터 레이크 아키텍처는 복수의 계층과 컴포넌트로 구성되어 있으며, 일반적으로 다음과 같은 주요 구성 요소를 포함합니다.

#### 데이터 수집 (Ingestion Layer)

이 계층에서는 다양한 소스에서 데이터를 수집합니다. 소스는 RDBMS, 파일 시스템, 로그 스트림, IoT 센서, API, 메시지 큐(Kafka, Kinesis) 등 다양할 수 있으며, 처리되지 않은 원시 데이터를 실시간 스트리밍 또는 일괄 처리(Batch) 방식으로 수집합니다.

예를 들어, AWS에서는 Kinesis Data Firehose를 사용하여 실시간 로그를 S3에 저장하고, Azure에서는 Event Hub를 통해 비슷한 인프라를 구현할 수 있습니다.

#### 데이터 저장소 (Storage Layer)

수집된 데이터는 데이터 레이크의 중심이 되는 대용량 스토리지에 저장됩니다. 이 계층은 Hadoop 분산파일시스템(HDFS)를 사용할 수도 있지만, 클라우드에서는 객체 스토리지가 일반적으로 선택됩니다. 이 스토리지 시스템은 예외적으로 다양한 형태의 파일(CSV, JSON, Parquet, Avro, 이미지, 비디오 등)을 저장할 수 있어야 하며, 보통 이런 스토리지 시스템은 99.999999999%(11 9's)의 내구성 보장을 제공합니다.

고성능 분석을 위해서는 데이터를 Parquet 또는 ORC와 같은 컬럼지향 포맷으로 변환하여 저장하는 것이 일반적입니다.

#### 데이터 처리 및 정제 (Processing & ETL Layer)

이 계층은 저장된 원시 데이터를 분석 가능하도록 정제하는 담당 역할을 수행합니다. 전통적인 ETL(Extract-Transform-Load) 방식이나 ELT(Extract-Load-Transform) 방식 모두 적용될 수 있으며, Apache Spark, AWS Glue, Azure Data Factory, Google Dataflow와 같은 분산 처리 플랫폼이 주로 사용됩니다.

이 계층에서는 중복 데이터 제거, 스키마 정리, 불필요한 컬럼 필터링, 날짜 포맷 정규화 등 데이터 정제 작업을 수행합니다. ML 파이프라인을 위한 featurization(특징 추출)이나 통합 ID 매핑과 같은 고차원 연산도 자주 수행됩니다.

#### 데이터 인덱싱 및 카탈로그 (Metadata & Governance Layer)

모든 데이터 레이크에는 메타데이터 관리 기능이 필수적입니다. 이 계층에서는 데이터의 스키마, 버전, 소스, 접근 권한, 수집 시점 등을 관리하며, 중앙 카탈로그(Schema Registry 또는 Data Catalog)를 통해 사용자나 애플리케이션이 해당 데이터를 효율적으로 탐색하고 조회할 수 있도록 지원합니다.

AWS Glue Data Catalog, Azure Purview 또는 Apache Hive Metastore 등이 이 용도로 활용됩니다.

#### 데이터 소비 및 분석 (Consumption Layer)

정제된 데이터는 다양한 방식으로 소비됩니다. 주로, BI 도구(Power BI, Tableau, Looker), SQL 분석 플랫폼(Athena, BigQuery, Presto), 머신러닝 프레임워크(SageMaker, Vertex AI, Azure ML), 또는 검색 시스템(OpenSearch, ElasticSearch) 등을 통해 데이터를 조회하고 분석합니다.

대규모 병렬 처리를 통해 수천 테라바이트의 데이터도 빠르게 처리할 수 있도록 서빙 계층에서의 성능 최적화도 중요한 부분입니다.

### 데이터 레이크와 데이터 웨어하우스의 비교

두 아키텍처는 서로 보완적인 관계를 가지며, 사용 목적과 데이터 특성에 따라 선택 또는 조합되어야 합니다. 아래는 비교 요약입니다.

| 항목 | 데이터 레이크 | 데이터 웨어하우스 |
|------|----------------|-------------------|
| 스키마 적용 | 읽을 때(Schema-on-Read) | 쓸 때(Schema-on-Write) |
| 데이터 유형 | 구조화, 반정형, 비정형 모두 | 주로 구조화 데이터 |
| 저장소 비용 | 상대적으로 저렴 | 상대적으로 고가 |
| 확장성 | 매우 높음, 수평적 확장 | 비교적 제한적 |
| 분석 지연성 | 실시간 또는 지연 수용 가능 | 실시간 중심 아님 |
| 사용자 | 데이터 과학자, ML 엔지니어 | BI 분석가, 비즈니스 사용자 |

많은 기업에서는 이 둘을 결합한 데이터 레이크하우스(Data Lakehouse) 아키텍처를 도입하여 양자의 장점을 동시에 취하려는 시도를 하고 있습니다.

### 데이터 레이크 설계 시 고려사항

효율적인 데이터 레이크 구축을 위해서는 다음과 같은 주요 사항들을 반드시 고려해야 합니다.

1. 데이터 레이크의 목적 정의  
   데이터 레이크가 보유하게 될 데이터 유형과 소비 대상(애널리스트, 데이터 과학자 등)을 명확하게 정의해야 합니다.

2. 거버넌스와 보안  
   데이터가 아무렇게나 저장된다면 골치 아픈 스파게티가 되기 쉽습니다. 따라서 데이터 분류, 접근 통제, 감사 추적 등의 메커니즘이 선행되어야 하며, IAM 기반의 접근 제어가 매우 중요합니다.

3. 스토리지 레이아웃 및 포맷  
   디렉토리 계층 구조(hierarchical folder structure)를 명확히 유지하고, 컬럼 지향 포맷 사용을 통해 쿼리 성능을 향상시키는 것도 중요합니다.

4. 버저닝과 시간 기반 파티셔닝  
   시간에 따라 데이터가 누적되고, 버전 관리가 필요한 경우 적절한 파티셔닝 전략(예: year/month/day)을 적용하여 효율적 관리가 이루어져야 합니다.

5. 메타데이터 관리  
   전체 데이터의 탐색성과 관리성을 높이기 위해, 데이터 카탈로그의 도입은 필수적입니다. 데이터가 많을수록 카탈로그의 유무가 생산성에 직접적인 영향을 줍니다.

6. 성능 관리 및 최적화  
   쿼리 성능을 높이기 위한 인덱싱, 파일 포맷 최적화, 캐싱 전략 등을 반드시 고려해야 합니다.

### 클라우드 기반의 데이터 레이크 구현 사례

다음은 실제 클라우드 환경에서 데이터 레이크 패턴이 적용되는 대표 사례입니다.

- **Netflix**  
  Netflix는 수천만 명의 시청자로부터 발생하는 로그, 시청 행동, 콘텐츠 피드백 데이터를 S3 기반의 데이터 레이크에 원시 형태로 수집하고, Apache Spark 및 Presto를 사용하여 실시간 스트리밍 분석 및 추천 시스템 학습 데이터로 활용하고 있습니다.

- **Airbnb**  
  Airbnb는 데이터 웨어하우스(Redshift)와 데이터 레이크(S3 + Hive Metastore)를 병행 운영하면서 복합 SQL 분석 및 머신러닝 모델 학습을 수행하고 있습니다.

- **국내 금융권 사례**  
  국내 일부 은행은 콜센터, 모바일 앱, 지점 오프라인 기록 등 고객 접점 데이터를 전사적으로 수집하여 Hadoop 기반의 데이터 레이크에 저장하고, 그 위에서 고객 이탈 예측, 이상 거래 탐지 등에 머신러닝을 적용하고 있습니다.

### 마무리하며

데이터 레이크는 다양한 데이터를 통합하고 분석 가능 상태로 유지하기 위한 현대적 접근 방식입니다. 특히 클라우드 환경에서는 객체 스토리지, 서버리스 ETL, 자동화된 카탈로그 도구 등을 활용하여 빠르게 구축할 수 있으며, 정형에서 비정형 데이터까지 포괄할 수 있다는 점에서 매우 강력한 아키텍처 패턴이라 할 수 있습니다.

그러나 데이터 레이크가 파일 더미 이상의 가치를 갖기 위해서는 체계적인 스키마 관리, 보안, 거버넌스 계획이 전제되어야 하며, 이를 소홀히 하면 ‘데이터 스웜프(Data Swamp)’로 전락할 위험 역시 존재합니다. 따라서 기술뿐만 아니라 조직적 측면의 관리 역량까지 함께 요구된다고 볼 수 있습니다.